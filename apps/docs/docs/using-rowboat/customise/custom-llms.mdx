---
title: "Custom LLMs"
description: "How to use and configure custom LLMs in Rowboat."

---

<Note> This is currently only possible in the self hosted version of Rowboat</Note>

## Using custom LLM providers

By default, Rowboat uses OpenAI LLMs (gpt-4o, gpt-4.1, etc.) for both agents and copilot, when you export your OPENAI_API_KEY. 

However, you can also configure custom LLM providers (e.g. LiteLLM, OpenRouter) to use any of the hundreds of available LLMs beyond OpenAI, such as Claude, DeepSeek, Ollama LLMs and so on.

<Steps>
  <Step title="Set up your LLM provider">
    Configure your environment variables to point to your preferred LLM backend. Example using LiteLLM:

    ```bash
    export PROVIDER_BASE_URL=http://host.docker.internal:4000/
    export PROVIDER_API_KEY=sk-1234
    ```

    Rowboat uses <code>gpt-4.1</code> as the default model for agents and copilot. You can override these:

    ```bash
    export PROVIDER_DEFAULT_MODEL=claude-3-7-sonnet-latest
    export PROVIDER_COPILOT_MODEL=gpt-4o
    ```

    **Notes:**
    - Copilot is optimized for <code>gpt-4o</code>/<code>gpt-4.1</code>. We strongly recommend using these models for best results.
    - You can use different models for the copilot and each agent, but all must be from the same provider (e.g., LiteLLM).
    - Rowboat is provider-agnostic â€” any backend implementing the OpenAI messages format should work.
    - OpenAI-specific tools (like <code>web_search</code>) will not function with non-OpenAI providers. Remove such tools to avoid errors.
  </Step>

  <Step title="Clone the repository and start Rowboat Docker">
    Clone the Rowboat repo and spin it up locally:

    ```bash
    git clone git@github.com:rowboatlabs/rowboat.git
    cd rowboat
    docker-compose up --build
    ```
  </Step>

  <Step title="Access the app">
    Once Docker is running, navigate to:

    [http://localhost:3000](http://localhost:3000)
  </Step>
</Steps>
